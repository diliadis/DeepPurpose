{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepPurpose Deep Dive\n",
    "## Tutorial 1: Training a Drug-Target Interaction Model from Scratch\n",
    "#### [@KexinHuang5](https://twitter.com/KexinHuang5)\n",
    "\n",
    "In this tutorial, we take a deep dive into DeepPurpose and show how it builds a drug-target interaction model from scratch. \n",
    "\n",
    "Agenda:\n",
    "\n",
    "- Part I: Overview of DeepPurpose and Data\n",
    "- Part II: Drug Target Interaction Prediction\n",
    "    - DeepPurpose Framework\n",
    "    - Applications to Drug Repurposing and Virtual Screening\n",
    "    - Pretrained Models\n",
    "    - Hyperparameter Tuning\n",
    "    - Model Robustness Evaluation\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/dimitriosi_datasets/anaconda3/envs/DeepPurpose_env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from DeepPurpose import utils, dataset\n",
    "from DeepPurpose import DTI as models\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Overview of DeepPurpose and Data\n",
    "\n",
    "Drug-target interaction measures the binding of drug molecules to the protein targets. Accurate identification of DTI is fundamental for drug discovery and supports many downstream tasks. Among others, drug screening and repurposing are two main applications based on DTI. Drug screening helps identify ligand candidates that can bind to the protein of interest, whereas drug repurposing finds new therapeutic purposes for existing drugs. Both tasks could alleviate the costly, time-consuming, and labor-intensive process of synthesis and analysis, which is extremely important, especially in the cases of hunting effective and safe treatments for COVID-19.\n",
    "\n",
    "DeepPurpose is a pytorch-based deep learning framework that is initiated to provide a simple but powerful toolkit for drug-target interaction prediction and its related applications. We see many exciting recent works in this direction, but to leverage these models, it takes lots of efforts due to the esoteric instructions and interface. DeepPurpose is designed to make things as simple as possible using a unified framework.\n",
    "\n",
    "DeepPurpose uses an encoder-decoder framework. Drug repurposing and screening are two applications after we obtain DTI models. The input to the model is a drug target pair, where drug uses the simplified molecular-input line-entry system (SMILES) string and target uses the amino acid sequence. The output is a score indicating the binding activity of the drug target pair. Now, we begin talking about the data format expected.\n",
    "\n",
    "\n",
    "(**Data**) DeepPurpose takes into an array of drug's SMILES strings (**d**), an array of target protein's amino acid sequence (**t**), and an array of label (**y**), which can either be binary 0/1 indicating interaction outcome or a real number indicating affinity value. The input drug and target arrays should be paired, i.e. **y**\\[0\\] is the score for **d**\\[0\\] and **t**\\[0\\].\n",
    "\n",
    "Besides transforming into numpy arrays through some data wrangling on your own, DeepPurpose also provides two ways to help data preparation. \n",
    "\n",
    "The first way is to read from local files. For example, to load drug target pairs, we expect a file.txt where each line is a drug SMILES string, followed by a protein sequence, and an affinity score or 0/1 label:\n",
    "\n",
    "```CC1=C...C4)N MKK...LIDL 7.365``` \\\n",
    "```CC1=C...C4)N QQP...EGKH 4.999```\n",
    "\n",
    "Then, we use ```dataset.read_file_training_dataset_drug_target_pairs``` to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug 1: CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N\n",
      "Target 1: MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQVTVDEVLAEGGFAIVFLVRTSNGMKCALKRMFVNNEHDLQVCKREIQIMRDLSGHKNIVGYIDSSINNVSSGDVWEVLILMDFCRGGQVVNLMNQRLQTGFTENEVLQIFCDTCEAVARLHQCKTPIIHRDLKVENILLHDRGHYVLCDFGSATNKFQNPQTEGVNAVEDEIKKYTTLSYRAPEMVNLYSGKIITTKADIWALGCLLYKLCYFTLPFGESQVAICDGNFTIPDNSRYSQDMHCLIRYMLEPDPDKRPDIYQVSYFSFKLLKKECPIPNVQNSPIPAKLPEPVKASEAAAKKTQPKARLTDPIPTTETSIAPRQRPKAGQTQPNPGILPIQPALTPRKRATVQPPPQAAGSSNQPGLLASVPQPKPQAPPSQPLPQTQAKQPQAPPTPQQTPSTQAQGLPAQAQATPQHQQQLFLKQQQQQQQPPPAQQQPAGTFYQQQQAQTQQFQAVHPATQKPAIAQFPVVSQGGSQQQLMQNFYQQQQQQQQQQQQQQLATALHQQQLMTQQAALQQKPTMAAGQQPQPQPAAAPQPAPAQEPAIQAPVRQQPKVQTTPPPAVQGQKVGSLTPPSSPKTQRAGHRRILSDVTHSAVFGVPASKSTQLLQAAAAEASLNKSKSATTTPSGSPRTSQQNVYNPSEGSTWNPFDDDNFSKLTAEELLNKDFAKLGEGKHPEKLGGSAESLIPGFQSTQGDAFATTSFSAGTAEKRKGGQTVDSGLPLLSVSDPFIPLQVPDAPEKLIEGLKSPDTSLLLPDLLPMTDPFGSTSDAVIEKADVAVESLIPGLEPPVPQRLPSQTESVTSNRTDSLTGEDSLLDCSLLSNPTTDLLEEFAPTAISAPVHKAAEDSNLISGFDVPEGSDKVAEDEFDPIPVLITKNPQGGHSRNSSGSSESSLPNLARSLLLVDQLIDL\n",
      "Score 1: 7.365\n"
     ]
    }
   ],
   "source": [
    "X_drugs, X_targets, y = dataset.read_file_training_dataset_drug_target_pairs('./toy_data/dti.txt')\n",
    "print('Drug 1: ' + X_drugs[0])\n",
    "print('Target 1: ' + X_targets[0])\n",
    "print('Score 1: ' + str(y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many method researchers want to test on benchmark datasets such as KIBA/DAVIS/BindingDB, DeepPurpose also provides data loaders to ease preprocessing. For example, we want to load the DAVIS dataset, we can use ```dataset.load_process_DAVIS```. It will download, preprocess to the designated data format. It supports label log-scale transformation for easier regression and also allows label binarization given a customized threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Processing...\n",
      "Beginning to extract zip file...\n",
      "Default set to logspace (nM -> p) for easier regression\n",
      "Done!\n",
      "Drug 1: CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC=C4)N\n",
      "Target 1: MKKFFDSRREQGGSGLGSGSSGGGGSTSGLGSGYIGRVFGIGRQQVTVDEVLAEGGFAIVFLVRTSNGMKCALKRMFVNNEHDLQVCKREIQIMRDLSGHKNIVGYIDSSINNVSSGDVWEVLILMDFCRGGQVVNLMNQRLQTGFTENEVLQIFCDTCEAVARLHQCKTPIIHRDLKVENILLHDRGHYVLCDFGSATNKFQNPQTEGVNAVEDEIKKYTTLSYRAPEMVNLYSGKIITTKADIWALGCLLYKLCYFTLPFGESQVAICDGNFTIPDNSRYSQDMHCLIRYMLEPDPDKRPDIYQVSYFSFKLLKKECPIPNVQNSPIPAKLPEPVKASEAAAKKTQPKARLTDPIPTTETSIAPRQRPKAGQTQPNPGILPIQPALTPRKRATVQPPPQAAGSSNQPGLLASVPQPKPQAPPSQPLPQTQAKQPQAPPTPQQTPSTQAQGLPAQAQATPQHQQQLFLKQQQQQQQPPPAQQQPAGTFYQQQQAQTQQFQAVHPATQKPAIAQFPVVSQGGSQQQLMQNFYQQQQQQQQQQQQQQLATALHQQQLMTQQAALQQKPTMAAGQQPQPQPAAAPQPAPAQEPAIQAPVRQQPKVQTTPPPAVQGQKVGSLTPPSSPKTQRAGHRRILSDVTHSAVFGVPASKSTQLLQAAAAEASLNKSKSATTTPSGSPRTSQQNVYNPSEGSTWNPFDDDNFSKLTAEELLNKDFAKLGEGKHPEKLGGSAESLIPGFQSTQGDAFATTSFSAGTAEKRKGGQTVDSGLPLLSVSDPFIPLQVPDAPEKLIEGLKSPDTSLLLPDLLPMTDPFGSTSDAVIEKADVAVESLIPGLEPPVPQRLPSQTESVTSNRTDSLTGEDSLLDCSLLSNPTTDLLEEFAPTAISAPVHKAAEDSNLISGFDVPEGSDKVAEDEFDPIPVLITKNPQGGHSRNSSGSSESSLPNLARSLLLVDQLIDL\n",
      "Score 1: 7.366531544420414\n"
     ]
    }
   ],
   "source": [
    "X_drugs, X_targets, y = dataset.load_process_DAVIS(path = './data', binary = False, convert_to_log = True, threshold = 30)\n",
    "print('Drug 1: ' + X_drugs[0])\n",
    "print('Target 1: ' + X_targets[0])\n",
    "print('Score 1: ' + str(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30056,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_drugs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30056,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30056,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detailed examples and tutorials of data loading, checkout this [tutorial](./DEMO/load_data_tutorial.ipynb).\n",
    "\n",
    "## Part II: Drug Target Interaction Prediction Framework\n",
    "\n",
    "DeepPurpose provides a simple framework to conduct DTI research using 8 encoders for drugs and 7 for proteins. It basically consists of the following steps, where each step corresponds to one line of code:\n",
    "\n",
    "- Encoder specification\n",
    "- Data encoding and split\n",
    "- Model configuration generation\n",
    "- Model initialization\n",
    "- Model Training\n",
    "- Model Prediction and Repuposing/Screening\n",
    "- Model Saving and Loading\n",
    "\n",
    "Let's start with data encoding! \n",
    "\n",
    "(**Encoder specification**) After we obtain the required data format from Part I, we need to prepare them for the encoders. Hence, we first specify the encoder to use for drug and protein. Here we try MPNN for drug and CNN for target.\n",
    "\n",
    "If you find MPNN and CNN are too large for the CPUs, you can try smaller encoders by uncommenting the last line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_encoding, target_encoding = 'MPNN', 'CNN'\n",
    "#drug_encoding, target_encoding = 'Morgan', 'Conjoint_triad'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can switch encoder just by changing the encoding name above. The full list of encoders are listed [here](https://github.com/kexinhuang12345/DeepPurpose#encodings). Here, we are using the message passing neural network encoder for drug and convolutional neural network encoder for protein.\n",
    "\n",
    "(**Data encoding and split**) Now, we encode the data into the specified format, using ```utils.data_process``` function. It specifies train/validation/test split fractions, and random seed to ensure same data splits for reproducibility. This function also support data splitting methods such as ```cold_drug``` and ```cold_protein```, which splits on drug/proteins for model robustness evaluation to test on unseen drug/proteins.\n",
    "\n",
    "The function outputs train, val, test pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug Target Interaction Prediction Mode...\n",
      "in total: 30056 drug-target pairs\n",
      "encoding drug...\n",
      "unique drugs: 68\n",
      "encoding protein...\n",
      "unique target sequence: 379\n",
      "splitting dataset...\n",
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Target Sequence</th>\n",
       "      <th>Label</th>\n",
       "      <th>drug_encoding</th>\n",
       "      <th>target_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...</td>\n",
       "      <td>PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>[[[tensor(1.), tensor(0.), tensor(0.), tensor(...</td>\n",
       "      <td>[P, F, W, K, I, L, N, P, L, L, E, R, G, T, Y, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              SMILES  \\\n",
       "0  CC1=C2C=C(C=CC2=NN1)C3=CC(=CN=C3)OCC(CC4=CC=CC...   \n",
       "\n",
       "                                     Target Sequence  Label  \\\n",
       "0  PFWKILNPLLERGTYYYFMGQQPGKVLGDQRRPSLPALHFIKGAGK...    5.0   \n",
       "\n",
       "                                       drug_encoding  \\\n",
       "0  [[[tensor(1.), tensor(0.), tensor(0.), tensor(...   \n",
       "\n",
       "                                     target_encoding  \n",
       "0  [P, F, W, K, I, L, N, P, L, L, E, R, G, T, Y, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val, test = utils.data_process(X_drugs, X_targets, y, \n",
    "                                drug_encoding, target_encoding, \n",
    "                                split_method='random',frac=[0.7,0.1,0.2],\n",
    "                                random_seed = 1)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Model configuration generation**) Now, we initialize a model with its configuration. You can modify almost any hyper-parameters (e.g., learning rate, epoch, batch size), model parameters (e.g. hidden dimensions, filter size) and etc in this function. The supported configurations are listed here in this [link](https://github.com/kexinhuang12345/DeepPurpose/blob/e169e2f550694145077bb2af95a4031abe400a77/DeepPurpose/utils.py#L486).\n",
    "\n",
    "For the sake of example, we specify the epoch size to be 5, and set the model parameters to be small so that you can run on both CPUs & GPUs quickly and can proceed to the next steps. For a reference parameters, checkout the notebooks in the DEMO folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = utils.generate_config(drug_encoding = drug_encoding, \n",
    "                        target_encoding = target_encoding, \n",
    "                        cls_hidden_dims = [1024,1024,512], \n",
    "                        train_epoch = 5, \n",
    "                        LR = 0.001, \n",
    "                        batch_size = 128,\n",
    "                        hidden_dim_drug = 128,\n",
    "                        hidden_dim_protein = 128,\n",
    "                        mpnn_hidden_size = 128,\n",
    "                        mpnn_depth = 3, \n",
    "                        cnn_target_filters = [32,64,96],\n",
    "                        cnn_target_kernels = [4,8,12],\n",
    "                        general_architecture_version = 'mlp',\n",
    "                        cuda_id='6',\n",
    "                        wandb_project_name = 'DeepPurpose',\n",
    "\t\t\t\t\t    wandb_project_entity = 'diliadis',\n",
    "                        use_early_stopping = True,\n",
    "\t\t\t\t\t    patience = 5,\n",
    "\t\t\t\t\t    delta = 0.001,\n",
    "\t\t\t\t\t    metric_to_optimize_early_stopping = 'loss',\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_dim_drug': 1024,\n",
       " 'input_dim_protein': 8420,\n",
       " 'hidden_dim_drug': 128,\n",
       " 'hidden_dim_protein': 128,\n",
       " 'cls_hidden_dims': [1024, 1024, 512],\n",
       " 'batch_size': 128,\n",
       " 'train_epoch': 5,\n",
       " 'test_every_X_epoch': 20,\n",
       " 'LR': 0.001,\n",
       " 'drug_encoding': 'MPNN',\n",
       " 'target_encoding': 'CNN',\n",
       " 'result_folder': './result/',\n",
       " 'binary': False,\n",
       " 'num_workers': 0,\n",
       " 'cuda_id': '6',\n",
       " 'general_architecture_version': 'mlp',\n",
       " 'experiment_name': None,\n",
       " 'wandb_project_name': 'DeepPurpose',\n",
       " 'wandb_project_entity': 'diliadis',\n",
       " 'use_early_stopping': True,\n",
       " 'patience': 5,\n",
       " 'delta': 0.001,\n",
       " 'metric_to_optimize_early_stopping': 'loss',\n",
       " 'metric_to_optimize_best_epoch_selection': 'loss',\n",
       " 'mpnn_hidden_size': 128,\n",
       " 'mpnn_depth': 3,\n",
       " 'cnn_target_filters': [32, 64, 96],\n",
       " 'cnn_target_kernels': [4, 8, 12]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Model initialization**) Next, we initialize a model using the above configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following device: cuda:6\n",
      "Using the MLP version of the architecture...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DeepPurpose.DTI.DBTA at 0x7f146b5ce350>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.model_initialize(**config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_Classifier(\n",
       "  (model_drug): MPNN(\n",
       "    (W_i): Linear(in_features=50, out_features=128, bias=False)\n",
       "    (W_h): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (W_o): Linear(in_features=167, out_features=128, bias=True)\n",
       "  )\n",
       "  (model_protein): CNN(\n",
       "    (conv): ModuleList(\n",
       "      (0): Conv1d(26, 32, kernel_size=(4,), stride=(1,))\n",
       "      (1): Conv1d(32, 64, kernel_size=(8,), stride=(1,))\n",
       "      (2): Conv1d(64, 96, kernel_size=(12,), stride=(1,))\n",
       "    )\n",
       "    (fc1): Linear(in_features=96, out_features=128, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (predictor): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**Model Training**) Next, it is ready to train, using the ```model.train``` function! If you do not have test set, you can just use ```model.train(train, val)```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdiliadis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/dimitriosi_datasets/DeepPurpose/wandb/run-20221006_213540-24hkmg6p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/diliadis/DeepPurpose/runs/24hkmg6p\" target=\"_blank\">lemon-plant-10</a></strong> to <a href=\"https://wandb.ai/diliadis/DeepPurpose\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Preparation ---\n",
      "--- Go for Training ---\n",
      "Training at Epoch 1 iteration 0 with loss 29.2203. Total time 0.0 hours\n",
      "Training at Epoch 1 iteration 100 with loss 1.10130. Total time 0.01166 hours\n",
      "Validation at Epoch 1 with loss:0.39055, MSE: 0.78777 , Pearson Correlation: 0.25437 with p-value: 1.32E-45 , Concordance Index: 0.62580\n",
      "Training at Epoch 2 iteration 0 with loss 0.93652. Total time 0.02138 hours\n",
      "Training at Epoch 2 iteration 100 with loss 0.77707. Total time 0.03305 hours\n",
      "Validation at Epoch 2 with loss:0.96737, MSE: 0.75909 , Pearson Correlation: 0.35641 with p-value: 9.73E-91 , Concordance Index: 0.67717\n",
      "-----------------------------EarlyStopping counter: 1 out of 5---------------------- best epoch currently 0\n",
      "Training at Epoch 3 iteration 0 with loss 0.94964. Total time 0.0425 hours\n",
      "Training at Epoch 3 iteration 100 with loss 0.96878. Total time 0.05444 hours\n",
      "Validation at Epoch 3 with loss:0.65576, MSE: 0.76386 , Pearson Correlation: 0.38056 with p-value: 3.35E-104 , Concordance Index: 0.70349\n",
      "-----------------------------EarlyStopping counter: 2 out of 5---------------------- best epoch currently 0\n",
      "Training at Epoch 4 iteration 0 with loss 0.95558. Total time 0.06388 hours\n",
      "Training at Epoch 4 iteration 100 with loss 0.61448. Total time 0.07583 hours\n",
      "Validation at Epoch 4 with loss:1.01725, MSE: 0.78502 , Pearson Correlation: 0.40300 with p-value: 8.86E-118 , Concordance Index: 0.72091\n",
      "-----------------------------EarlyStopping counter: 3 out of 5---------------------- best epoch currently 0\n",
      "Training at Epoch 5 iteration 0 with loss 0.69688. Total time 0.08527 hours\n",
      "Training at Epoch 5 iteration 100 with loss 0.84496. Total time 0.09722 hours\n",
      "Validation at Epoch 5 with loss:0.66290, MSE: 0.69062 , Pearson Correlation: 0.40036 with p-value: 4.01E-116 , Concordance Index: 0.71400\n",
      "-----------------------------EarlyStopping counter: 4 out of 5---------------------- best epoch currently 0\n",
      "Best params!!! {'val_MSE': 0.6906204654991948, 'val_pearson_correlation': 0.40036137751848544, 'val_concordance_index': 0.7140081099931573, 'val_loss': 0.6629071831703186}\n",
      "--- Go for Testing ---\n",
      "Testing MSE: 0.7007403330745505 , Pearson Correlation: 0.3799342075465042 with p-value: 1.04E-205 , Concordance Index: 0.7043515030661445\n",
      "--- Training Finished ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>▁▃▅▆█</td></tr><tr><td>best_val_MSE</td><td>▁</td></tr><tr><td>best_val_concordance_index</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr><tr><td>best_val_pearson_correlation</td><td>▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test_MSE</td><td>▁</td></tr><tr><td>test_concordance_index</td><td>▁</td></tr><tr><td>test_pearson_correlation</td><td>▁</td></tr><tr><td>train_batch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁</td></tr><tr><td>val_MSE</td><td>█▆▆█▁</td></tr><tr><td>val_concordance_index</td><td>▁▅▇█▇</td></tr><tr><td>val_loss</td><td>▁▇▄█▄</td></tr><tr><td>val_pearson_correlation</td><td>▁▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>825</td></tr><tr><td>best_val_MSE</td><td>0.69062</td></tr><tr><td>best_val_concordance_index</td><td>0.71401</td></tr><tr><td>best_val_loss</td><td>0.66291</td></tr><tr><td>best_val_pearson_correlation</td><td>0.40036</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>test_MSE</td><td>0.70074</td></tr><tr><td>test_concordance_index</td><td>0.70435</td></tr><tr><td>test_pearson_correlation</td><td>0.37993</td></tr><tr><td>train_batch</td><td>825</td></tr><tr><td>train_loss</td><td>0.78113</td></tr><tr><td>val_MSE</td><td>0.69062</td></tr><tr><td>val_concordance_index</td><td>0.71401</td></tr><tr><td>val_loss</td><td>0.66291</td></tr><tr><td>val_pearson_correlation</td><td>0.40036</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lemon-plant-10</strong>: <a href=\"https://wandb.ai/diliadis/DeepPurpose/runs/24hkmg6p\" target=\"_blank\">https://wandb.ai/diliadis/DeepPurpose/runs/24hkmg6p</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221006_213540-24hkmg6p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train(train, val, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DeepPurpose_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "25394f763b65794906a1520a5e350de27236fec0e3f27326c170a0c4ecd2d822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
